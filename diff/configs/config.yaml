defaults:
  - _self_
  - model: small
  - override hydra/launcher: submitit_slurm


dataset_name: ${oc.env:DATASET_NAME,null} # can be overridden by environment variable
train_size: ${oc.env:TRAIN_SIZE,null}  # Can be overridden by environment variable
n_iters: ${oc.env:N_ITERS,null} # Can be overridden by environment variable
work_dir: exp_local/${data.train}_6/${now:%Y.%m.%d}/${now:%H%M%S}_size${oc.select:train_size,full}

ngpus: 8
tokens: 50257
pad_token_id: 50256

training:
  batch_size: 64
  accum: 1
  n_iters: 150000 # CHANGE HERE - HOW MANY ITERATIONS YOU WANT TO RUN 30K TO 100K
  snapshot_freq: 500
  log_freq: 100
  eval_freq: 50000
  snapshot_freq_for_preemption: 10000
  weight: standard
  snapshot_sampling: True
  ema: 0.9999

#CHANGE HERE  THE DATASET NAME

data:
  train: ${dataset_name}
  valid: ${dataset_name}
  # train: Sp1786/multiclass-sentiment-analysis-dataset
  # valid: Sp1786/multiclass-sentiment-analysis-dataset

  cache_dir: data

graph:
  type: absorb
  file: data
  report_all: False

noise:
  type: loglinear
  sigma_min: 1e-4
  sigma_max: 20

sampling:
  predictor: euler
  steps: 128
  noise_removal: True

eval:
  batch_size: 64
  perplexity: True
  perplexity_batch_size: 4

optim:
  weight_decay: 0
  optimizer: AdamW
  lr: 3e-4 # added by foobar
  # lr: 3e-5 
  beta1: 0.9
  beta2: 0.999
  # eps: 1e-8 # added by foobar
  eps: 1e-8
  warmup: 2500
  grad_clip: 1.


hydra:
  run:
    dir: ${work_dir}
  sweep:
    dir: exp/${data.train}_6/${now:%Y.%m.%d}/${now:%H%M%S}_size${oc.env:TRAIN_SIZE,full}
    subdir: ${hydra.job.num}
  launcher:
    max_num_timeout: 100000
    # timeout_min: 10079
    partition: g40x
    account: stanford
    mem_gb: 96
    cpus_per_task: 40
    gpus_per_node: ${ngpus}
    constraint: null
